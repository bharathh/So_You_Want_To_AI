{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a6b6d25",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# üß≠ Phase 4: Agent Workflows & Multi-Agent Design\n",
        "\n",
        "In Phase 3 you built a **single agent** that could use tools (via MCP). In this phase, you‚Äôll learn how to design **agent workflows** and build a simple **multi-agent workflow** in code.\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 14px; border-radius: 8px; border-left: 4px solid #2e7d32;\">\n",
        "<strong>üéØ What you'll do:</strong> Understand common agent workflow patterns, learn how to pick the right one for your use case and tech stack, design personas and human-in-the-loop checkpoints, and finally implement a small multi-agent workflow using Groq + MCP.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Recap & segue from Phase 3** ‚Äî From single agent to workflows\n",
        "- **Agent workflow types** ‚Äî Sequential, parallel, and coordinated patterns in detail\n",
        "- **How to choose a workflow type** ‚Äî Questions to ask; concrete examples where one type beats another\n",
        "- **Exercise** ‚Äî Choose the best workflow type for given scenarios (with suggested answers)\n",
        "- **Designing a workflow** ‚Äî Breaking work into phases and personas (step-by-step)\n",
        "- **Framework choices** ‚Äî How LangChain, LlamaIndex, crewAI, AutoGen differ\n",
        "- **Human-in-the-loop (HITL)** ‚Äî Where to place humans; design choices (granularity, blocking vs async)\n",
        "- **Code lab** ‚Äî Implement a simple multi-agent workflow with Groq + MCP\n",
        "- **Exercises (end of phase)** ‚Äî Sequential vs parallel, HITL placement, two-persona design\n",
        "- **Phase 4 wrap-up** ‚Äî Summary and next steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÅ From a single agent (Phase 3) to workflows\n",
        "\n",
        "In **Phase 3**, you:\n",
        "- Wrapped Groq‚Äôs `responses.create` + an MCP tool into a **single-agent helper**.\n",
        "- Let the model **decide when to call the Hugging Face MCP tool**.\n",
        "- Inspected `mcp_call` items to see how the agent used tools.\n",
        "\n",
        "That‚Äôs already a minimal **agent**: one loop around a model that can decide to use tools.\n",
        "\n",
        "In this phase we zoom out and design a **workflow**:\n",
        "- Multiple **steps/phases** (e.g. research ‚Üí evaluation ‚Üí decision).\n",
        "- Optionally multiple **personas/agents**, each with their own role and tools.\n",
        "- **Human-in-the-loop** checkpoints for safety and control.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283b9ef7",
      "metadata": {},
      "source": [
        "## üß± Agent workflow types (patterns)\n",
        "\n",
        "Workflows differ in **how agents are ordered** and **how they share information**. Three high-level categories:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Sequential workflows**\n",
        "\n",
        "Agents run **one after another** in a fixed order. Output of step *N* becomes input to step *N+1*.\n",
        "\n",
        "- **Flow:** `Agent A ‚Üí Agent B ‚Üí Agent C ‚Üí ‚Ä¶`\n",
        "- **Data:** Each step consumes the previous step‚Äôs output (and optionally the original user goal).\n",
        "- **Use when:** The task has a natural pipeline (e.g. research ‚Üí evaluate ‚Üí recommend), dependencies are clear, and you want predictable, auditable steps.\n",
        "- **Trade-offs:** Easy to reason about and debug; latency adds up (no parallelism). Best with a **single strong model** per step or **specialist models** (e.g. one for search, one for summarization).\n",
        "\n",
        "**Examples:** Document ingestion (parse ‚Üí extract ‚Üí classify), model selection (research ‚Üí evaluate ‚Üí recommend), content pipelines (research ‚Üí draft ‚Üí review).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Parallel workflows**\n",
        "\n",
        "Multiple agents (or branches) run **at the same time**, on the same or different inputs. Results are merged later.\n",
        "\n",
        "- **Flow:** Fork into *N* branches; each branch runs independently; then join (e.g. vote, rank, or concatenate).\n",
        "- **Data:** Same input broadcast to all branches, or different slices (e.g. by topic). Merge step combines outputs.\n",
        "- **Use when:** You need **diversity** (e.g. multiple draft answers to pick from), **coverage** (e.g. analyze 5 documents in parallel), or **redundancy** (e.g. multiple reviewers). Best when steps are **independent** and you can afford multiple model calls.\n",
        "- **Trade-offs:** Lower end-to-end latency than doing the same steps sequentially; higher cost per request. Merge logic (voting, ranking, summarization) must be designed carefully.\n",
        "\n",
        "**Examples:** Multi-perspective summarization (each agent summarizes one section in parallel, then merge), parallel code reviews (several agents review the same PR), A/B-style answer generation (generate 3 answers in parallel, then rank).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Coordinated workflows**\n",
        "\n",
        "A **coordinator** (or router) decides **which agent(s)** to call, in **what order**, and possibly **multiple rounds**. Flow is dynamic, not fixed.\n",
        "\n",
        "- **Flow:** Coordinator receives goal ‚Üí chooses next agent(s) and task(s) ‚Üí agents run (possibly in parallel) ‚Üí coordinator gets results ‚Üí repeats or finishes.\n",
        "- **Data:** Coordinator maintains state (e.g. plan, partial results, history); agents receive sub-tasks and return structured or free-form results.\n",
        "- **Use when:** The task is **open-ended** or **multi-step with branching** (e.g. ‚Äúanswer this question using whatever tools you need‚Äù), or when you want **one agent to delegate** to specialists. Fits **planner‚Äìexecutor** and **tool-using single agent** patterns.\n",
        "- **Trade-offs:** Flexible and powerful; harder to debug and more variable in cost/latency. Often implemented with a **single model that has access to tools** (including ‚Äúcall another agent‚Äù as a tool) or a **dedicated coordinator model** that outputs a plan.\n",
        "\n",
        "**Examples:** Research assistant (plan ‚Üí search ‚Üí read ‚Üí synthesize ‚Üí maybe search again), helpdesk (classify ‚Üí route to specialist agent ‚Üí maybe escalate), iterative refinement (draft ‚Üí critique ‚Üí revise until satisfied).\n",
        "\n",
        "---\n",
        "\n",
        "### Summary table\n",
        "\n",
        "| Pattern        | Order of execution | Info flow           | Best for                                      |\n",
        "|----------------|--------------------|---------------------|-----------------------------------------------|\n",
        "| **Sequential** | Fixed, step-by-step| A ‚Üí B ‚Üí C           | Pipelines, audits, clear dependencies         |\n",
        "| **Parallel**   | Multiple at once   | Fork ‚Üí merge        | Diversity, coverage, independent sub-tasks   |\n",
        "| **Coordinated**| Dynamic, by coordinator | Coordinator ‚Üî agents | Open-ended tasks, planning, delegation |\n",
        "\n",
        "---\n",
        "\n",
        "### Other patterns (brief)\n",
        "\n",
        "- **Single-call tool-using agent** ‚Äî One model call can use tools (like Phase 3). Good for small, well-scoped tasks; no explicit ‚Äúworkflow,‚Äù just one intelligent step.\n",
        "- **Planner‚Äìexecutor loop** ‚Äî A form of coordinated workflow: one agent outputs a plan; another (or the same) executes steps, possibly with tools, in a loop.\n",
        "- **Agent in a larger app** ‚Äî Any of the above embedded in APIs, queues, or schedulers; adds observability, retries, and SLAs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39e14013",
      "metadata": {},
      "source": [
        "## üéØ How to choose a workflow type\n",
        "\n",
        "Ask these questions before choosing a pattern:\n",
        "\n",
        "- **Task complexity** ‚Äî One-shot question vs. multi-step project with dependencies.\n",
        "- **Determinism vs exploration** ‚Äî Predictable path (pipelines) vs. flexible exploration (coordinated / planner).\n",
        "- **Latency & cost** ‚Äî How many model calls per request you can afford.\n",
        "- **Risk & safety** ‚Äî Whether you need human approval before certain actions (e.g. sending emails, committing code).\n",
        "- **Data & tools** ‚Äî Which MCP servers, APIs, or DBs must be used; local vs. remote.\n",
        "- **Team / org mapping** ‚Äî Whether you can mirror existing human roles (researcher, reviewer, approver) as agent personas.\n",
        "\n",
        "---\n",
        "\n",
        "### When one type is better than another: concrete examples\n",
        "\n",
        "Below are **concrete use cases** and why a given **workflow type** (and sometimes **model choice**) fits better.\n",
        "\n",
        "| Use case | Better fit | Why the other options are worse |\n",
        "|----------|------------|----------------------------------|\n",
        "| **‚ÄúFind a Hugging Face model for task X and recommend one‚Äù** | **Sequential** (research ‚Üí evaluate ‚Üí recommend) | Dependencies are clear: you must research before evaluating. Parallel doesn‚Äôt help (evaluation needs research output). Coordinated is overkill for this linear process. |\n",
        "| **‚ÄúSummarize these 10 long documents‚Äù** | **Parallel** (one agent per doc, then merge) | Each doc is independent; parallelizing cuts latency. Sequential would be slow (10 steps in a row). Coordinated adds complexity without benefit. |\n",
        "| **‚ÄúAnswer this question using the web and our internal docs; use whatever steps you need‚Äù** | **Coordinated** (single agent with tools, or planner‚Äìexecutor) | Steps are unknown in advance (search ‚Üí maybe read ‚Üí maybe search again). Sequential assumes a fixed pipeline; parallel assumes independent branches. A tool-using or planner agent fits best. |\n",
        "| **‚ÄúGenerate 5 tagline options for our product; pick the best one‚Äù** | **Parallel** (5 agents in parallel, then one ranker) | You want diversity (5 different options) then a single decision. Sequential would give one option per step and no direct comparison. Coordinated could do it but parallel is simpler and faster. |\n",
        "| **‚ÄúReview this PR: security, style, and performance‚Äù** | **Parallel** (3 specialist reviewers in parallel, then merge) | Three independent review dimensions; running in parallel is faster and keeps roles clear. Sequential would delay the later reviews; coordinated adds orchestration you don‚Äôt need. |\n",
        "| **‚ÄúDraft a report, then have a critic improve it, then have an editor polish it‚Äù** | **Sequential** (draft ‚Üí critique ‚Üí edit) | Each step depends on the previous. Parallel can‚Äôt work (critic needs draft; editor needs revised draft). Coordinated could emulate this but sequential is the natural fit. |\n",
        "| **‚ÄúHandle customer ticket: classify, then route to the right specialist, maybe escalate‚Äù** | **Coordinated** (classifier ‚Üí specialist agents; coordinator can re-route or escalate) | Routing and escalation are dynamic. Sequential would force a fixed path; parallel doesn‚Äôt model routing. A coordinator that chooses the next agent fits best. |\n",
        "| **‚ÄúOne-shot: what‚Äôs the weather in Paris?‚Äù** | **Single-call tool-using agent** (one model + one tool call) | No multi-step workflow; one call with a tool is enough. Sequential/parallel/coordinated add unnecessary complexity and cost. |\n",
        "\n",
        "---\n",
        "\n",
        "### Model architecture and workflow\n",
        "\n",
        "- **Sequential:** Often **one model per step** (same or different). You can use a smaller/faster model for simple steps (e.g. classification) and a larger one for complex steps (e.g. summarization).\n",
        "- **Parallel:** Same model replicated across branches is common; or **specialist models** per branch (e.g. security reviewer vs. style reviewer). Throughput and cost scale with the number of branches.\n",
        "- **Coordinated:** Often **one powerful model** with tools (including ‚Äúcall another agent‚Äù or ‚Äúrun sub-task‚Äù); or a **lightweight coordinator** that calls specialist models. The coordinator‚Äôs prompt and tool set define the possible behaviors.\n",
        "\n",
        "For this notebook, we‚Äôll implement a **sequential multi-agent pipeline** (research ‚Üí evaluate ‚Üí recommend) with clear personas and explicit human checkpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c72ff8",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Exercise: Choose the workflow type\n",
        "\n",
        "For each scenario below, decide which workflow type fits best: **sequential**, **parallel**, or **coordinated** (or **single-call** if it‚Äôs a one-shot task). Justify in one or two sentences. Optionally, sketch the steps (e.g. ‚ÄúAgent A ‚Üí Agent B ‚Üí merge‚Äù).\n",
        "\n",
        "1. **Scenario A:** ‚ÄúWe have 50 support tickets. For each ticket, classify intent (billing / technical / feedback), then generate a first draft reply. We want results as fast as possible.‚Äù\n",
        "2. **Scenario B:** ‚ÄúWe want one final blog post. First research 3 competitor articles, then draft an outline, then write the post, then run a fact-check pass.‚Äù\n",
        "3. **Scenario C:** ‚ÄúUser asks a free-form question. We want to use web search and our knowledge base, and we don‚Äôt know in advance how many searches or reads we‚Äôll need.‚Äù\n",
        "4. **Scenario D:** ‚ÄúWe need 3 alternative ad copy options for a campaign. We‚Äôll then pick one manually. No dependency between the three options.‚Äù\n",
        "5. **Scenario E:** ‚ÄúProcess incoming invoices: extract line items (OCR + parser), validate against PO database, then flag exceptions for human review.‚Äù\n",
        "\n",
        "**Suggested answers (try the exercise first, then peek):**\n",
        "\n",
        "- **A:** **Parallel** ‚Äî 50 independent tickets; classify + draft per ticket in parallel (or parallel over tickets) to minimize latency. Merge could be ‚Äúcollect all results‚Äù or a final QA pass.\n",
        "- **B:** **Sequential** ‚Äî research ‚Üí outline ‚Üí write ‚Üí fact-check. Each step depends on the previous; fixed pipeline.\n",
        "- **C:** **Coordinated** ‚Äî open-ended; number and order of tool calls (search, read) are unknown. Single agent with tools or planner‚Äìexecutor fits.\n",
        "- **D:** **Parallel** ‚Äî 3 independent draft agents in parallel; human picks one. No need for sequential or coordinator.\n",
        "- **E:** **Sequential** ‚Äî extract ‚Üí validate ‚Üí flag. Clear pipeline with dependencies; human-in-the-loop at the end for exceptions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e32d84b",
      "metadata": {},
      "source": [
        "## üßë‚Äçüéì Designing personas & phases\n",
        "\n",
        "Breaking a workflow into **phases** and **personas** makes behavior predictable and easier to debug. Here‚Äôs how to do it in more detail.\n",
        "\n",
        "---\n",
        "\n",
        "### What is a persona?\n",
        "\n",
        "A **persona** is a specialized agent with:\n",
        "\n",
        "- **Role** ‚Äî What it is responsible for (e.g. *Model Researcher*, *Evaluator*, *Writer*).\n",
        "- **Goals** ‚Äî How success is defined (e.g. *find 3‚Äì5 candidate models*, *score each from 1‚Äì10*).\n",
        "- **Tools** ‚Äî Which MCP servers, APIs, or data sources it can use (e.g. Hugging Face MCP, web search, internal DB).\n",
        "- **Constraints** ‚Äî Style, safety rules, output format, and limits (e.g. ‚Äúoutput markdown only,‚Äù ‚Äúdo not call external APIs without approval‚Äù).\n",
        "\n",
        "Personas map well to **existing human roles** (researcher, reviewer, approver), which helps with handoffs and human-in-the-loop.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-step design process\n",
        "\n",
        "1. **Define the end-to-end goal.**  \n",
        "   Be specific: ‚ÄúPick a Hugging Face text-generation model for marketing copy with low latency and a permissive license‚Äù is better than ‚Äúfind a good model.‚Äù\n",
        "\n",
        "2. **List the logical steps (phases).**  \n",
        "   - What must happen first? (e.g. gather options.)  \n",
        "   - What depends on that? (e.g. evaluate options against criteria.)  \n",
        "   - What comes last? (e.g. recommend and explain.)\n",
        "\n",
        "3. **Assign a persona to each phase.**  \n",
        "   - Give each persona a **name** (e.g. `ResearcherAgent`, `EvaluatorAgent`) and a **one-line responsibility**.  \n",
        "   - Decide **which tools** each persona can use. Not every agent needs tools (e.g. EvaluatorAgent might only reason over text).\n",
        "\n",
        "4. **Define inputs and outputs per step.**  \n",
        "   - ResearcherAgent: input = user goal; output = structured list of candidates.  \n",
        "   - EvaluatorAgent: input = user goal + research summary; output = scores + final recommendation.  \n",
        "   This clarifies handoffs and makes it easy to add human checkpoints (e.g. ‚Äúhuman approves research summary before it goes to EvaluatorAgent‚Äù).\n",
        "\n",
        "5. **Decide where humans must be involved.**  \n",
        "   - After research: human can refine scope or add constraints.  \n",
        "   - Before final recommendation: human can approve, reject, or request another iteration.  \n",
        "   - Optionally: human approves each tool call (e.g. `require_approval` for sensitive MCP tools).\n",
        "\n",
        "---\n",
        "\n",
        "### Example: our model-selection workflow\n",
        "\n",
        "| Phase   | Persona           | Input              | Output                    | Tools        |\n",
        "|---------|--------------------|--------------------|---------------------------|--------------|\n",
        "| 1       | ResearcherAgent    | User goal          | 3‚Äì5 candidates (summary)  | Hugging Face MCP |\n",
        "| (HITL)  | Human              | Research summary   | Approve / refine / stop   | ‚Äî            |\n",
        "| 2       | EvaluatorAgent     | Goal + research    | Scores + recommendation   | None         |\n",
        "| (HITL)  | Human              | Evaluation         | Approve / stop            | ‚Äî            |\n",
        "| 3       | Coordinator        | Approved output    | Final recommendation      | ‚Äî            |\n",
        "\n",
        "We implement ‚ÄúCoordinator‚Äù as the Python function that runs the pipeline and calls the human at each checkpoint; it doesn‚Äôt need a separate model call.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß∞ Framework choices: how they differ\n",
        "\n",
        "You can implement these workflows in different ways:\n",
        "\n",
        "| Framework / style | Strengths | When to use |\n",
        "|-------------------|-----------|-------------|\n",
        "| **Groq + MCP (DIY)** | Direct control over prompts, tools, and loops; minimal dependencies. | Teaching, prototypes, or when you want to understand the mechanics (like this notebook). |\n",
        "| **LangChain** | Rich library of tools, agents, memory, and chains; many integrations. | When you want batteries-included tooling and flexible orchestration in Python/JS. |\n",
        "| **LlamaIndex** | Strong focus on indexes, retrieval, and data-centric agents. | When your workflow centers on documents, RAG, and structured data sources. |\n",
        "| **crewAI** | Multi-agent ‚Äúcrew‚Äù with roles, goals, and tasks; built-in coordination patterns. | When you want opinionated multi-agent patterns with less manual orchestration. |\n",
        "| **AutoGen** | Conversational multi-agent framework (agents talk to each other). | When you want agents to negotiate, critique, or co-create via dialogue. |\n",
        "\n",
        "In this notebook we use **Groq + MCP directly** so you can see the underlying pattern that other frameworks abstract over.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a929f876",
      "metadata": {},
      "source": [
        "## üôã Human-in-the-loop (HITL): where to place humans\n",
        "\n",
        "Humans can appear at several points in an agent workflow. Choosing the **right places** balances autonomy (fewer interruptions) with safety and control (more oversight where it matters).\n",
        "\n",
        "---\n",
        "\n",
        "### Where to add HITL checkpoints\n",
        "\n",
        "| Checkpoint type      | When to use it | What the human does |\n",
        "|----------------------|----------------|----------------------|\n",
        "| **Task definition**  | Start of workflow | Provide or edit the initial goal, constraints, or scope. |\n",
        "| **Tool approval**    | Before sensitive tool calls | Approve or reject each call to tools that change state (e.g. send email, write to DB, call paid APIs). With MCP, this is often modeled via `require_approval` or connector settings. |\n",
        "| **Intermediate review** | After a phase (e.g. research, draft) | Inspect output; approve to continue, refine the prompt, or stop. Prevents bad intermediate results from flowing to the next agent. |\n",
        "| **Final approval**   | End of workflow | Approve the final recommendation or output before it is applied (e.g. before publishing, deploying, or sending to a customer). |\n",
        "| **Escalation**       | When the agent is uncertain | Agent explicitly hands control back (e.g. ‚ÄúI couldn‚Äôt find a match; please clarify‚Äù). Human resolves and may re-enter the workflow. |\n",
        "\n",
        "---\n",
        "\n",
        "### Design choices\n",
        "\n",
        "- **Granularity:** Approval per tool call (fine-grained, more friction) vs. approval per phase (coarser, fewer interruptions). Use per-phase for most workflows; use per-tool for high-risk actions.\n",
        "- **Blocking vs. async:** Blocking = workflow pauses until the human responds (e.g. console `input()`). Async = workflow continues and notifies a human (e.g. ticket, Slack); human can intervene later. Our code example uses **blocking** console prompts for simplicity.\n",
        "- **Default behavior:** If the human doesn‚Äôt respond in time, decide: auto-continue, auto-stop, or retry. Document this in your runbooks.\n",
        "\n",
        "In our code example we implement **two intermediate checkpoints** (after research, after evaluation) using simple **blocking console prompts** (y/n). You can replace these with UI buttons, tickets, or approval workflows in production.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Same as previous phases: install **openai**, set **GROQ_API_KEY** (and optional **HF_TOKEN**), and create a Groq client. Then we‚Äôll reuse the Hugging Face MCP tool from Phase 2‚Äì3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Paste your GROQ_API_KEY: \")\n",
        "if not os.environ.get(\"HF_TOKEN\"):\n",
        "    tok = getpass(\"Paste your HF_TOKEN (or press Enter to skip): \")\n",
        "    if tok:\n",
        "        os.environ[\"HF_TOKEN\"] = tok\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_groq_client():\n",
        "    return OpenAI(\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "    )\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "print(\"‚úÖ Groq client ready.\" if os.environ.get(\"GROQ_API_KEY\") else \"Need GROQ_API_KEY.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666ede92",
      "metadata": {},
      "source": [
        "## üîß MCP tool (Hugging Face) ‚Äî same as Phase 2‚Äì3\n",
        "\n",
        "We reuse the same Hugging Face MCP tool from Phase 2‚Äì3. Only the **ResearcherAgent** will use it; the **EvaluatorAgent** will reason over text only (no tools)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7761131",
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_tool = {\n",
        "    \"type\": \"mcp\",\n",
        "    \"server_label\": \"Huggingface\",\n",
        "    \"server_url\": \"https://huggingface.co/mcp\",\n",
        "    \"server_description\": \"Search and access AI models from Hugging Face\",\n",
        "    \"require_approval\": \"never\",  # for this lab we automate tool calls\n",
        "}\n",
        "if HF_TOKEN:\n",
        "    mcp_tool[\"headers\"] = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "print(\"MCP tool: Hugging Face\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Designing our example multi-agent workflow\n",
        "\n",
        "We‚Äôll implement a **3-persona workflow** around Hugging Face model selection:\n",
        "\n",
        "- **User goal** (input): e.g. ‚ÄúI need a text-generation model for marketing copy with low latency and a permissive license.‚Äù\n",
        "- **Agent 1 ‚Äì ResearcherAgent**: uses Hugging Face MCP to find 3‚Äì5 candidate models and summarize them.\n",
        "- **Human checkpoint #1**: you review/refine the research summary.\n",
        "- **Agent 2 ‚Äì EvaluatorAgent**: takes the refined research + requirements and scores/ranks candidates.\n",
        "- **Human checkpoint #2**: you approve the evaluation or ask for another pass.\n",
        "- **Coordinator**: packages a final recommendation.\n",
        "\n",
        "We‚Äôll keep all of this in one notebook and implement agents as **small helper functions** around `responses.create`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = get_groq_client()\n",
        "\n",
        "def run_agent(role_name: str, role_instructions: str, task: str, tools=None):\n",
        "    \"\"\"Generic helper to run an agent with a role, instructions, and a task.\n",
        "\n",
        "    - role_name: label used in logs (e.g. \"ResearcherAgent\").\n",
        "    - role_instructions: how this agent should behave.\n",
        "    - task: natural-language task description.\n",
        "    - tools: list of tools this agent can use (e.g. [mcp_tool]).\n",
        "    \"\"\"\n",
        "    full_prompt = (\n",
        "        f\"You are {role_name}.\\n\"  # light system-style prefix\n",
        "        f\"{role_instructions}\\n\\n\"\n",
        "        f\"Task: {task}\"\n",
        "    )\n",
        "    response = client.responses.create(\n",
        "        model=\"openai/gpt-oss-120b\",\n",
        "        input=full_prompt,\n",
        "        tools=tools or [],\n",
        "    )\n",
        "    return response\n",
        "\n",
        "print(\"‚úÖ Agent helper ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90305ee3",
      "metadata": {},
      "source": [
        "## üß™ Multi-agent workflow with human-in-the-loop\n",
        "\n",
        "The function below implements our **sequential** pipeline: ResearcherAgent (with MCP) ‚Üí human checkpoint ‚Üí EvaluatorAgent (no tools) ‚Üí human checkpoint ‚Üí final recommendation. Set `hitl=False` to run without console prompts (e.g. for automated demos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59eb5d20",
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_multi_agent_workflow(user_goal: str, hitl: bool = True):\n",
        "    \"\"\"Run a simple Researcher ‚Üí Evaluator pipeline with optional human approval.\n",
        "\n",
        "    Returns the final recommendation text.\n",
        "    \"\"\"\n",
        "    print(\"=== USER GOAL ===\")\n",
        "    print(user_goal)\n",
        "    print(\"\\n=== STEP 1: ResearcherAgent (uses Hugging Face MCP) ===\")\n",
        "\n",
        "    researcher_instructions = (\n",
        "        \"You are a model researcher using the Hugging Face Hub. \"\n",
        "        \"Use the available tools to find 3‚Äì5 candidate models that could meet the user's goal. \"\n",
        "        \"For each candidate, capture: model id, task, license, rough size, and why it might fit. \"\n",
        "        \"Return your findings as a concise markdown bullet list.\"\n",
        "    )\n",
        "\n",
        "    researcher_resp = run_agent(\n",
        "        role_name=\"ResearcherAgent\",\n",
        "        role_instructions=researcher_instructions,\n",
        "        task=user_goal,\n",
        "        tools=[mcp_tool],\n",
        "    )\n",
        "\n",
        "    researcher_summary = researcher_resp.output_text\n",
        "    print(\"\\n--- ResearcherAgent output ---\\n\")\n",
        "    print(researcher_summary)\n",
        "\n",
        "    if hitl:\n",
        "        cont = input(\"\\nApprove research summary? [y to continue, anything else to stop]: \")\n",
        "        if cont.strip().lower() != \"y\":\n",
        "            print(\"\\n‚èπ Workflow stopped by human after research phase.\")\n",
        "            return researcher_summary\n",
        "\n",
        "    print(\"\\n=== STEP 2: EvaluatorAgent (scores candidates vs constraints) ===\")\n",
        "\n",
        "    evaluator_instructions = (\n",
        "        \"You are an evaluation expert. Given the user's goal and the researcher's \"\n",
        "        \"candidate list, score each candidate from 1‚Äì10 for fit. \"\n",
        "        \"Consider task match, license, community adoption, and likely latency. \"\n",
        "        \"Return: (1) a short table of candidates with scores, (2) a clear final recommendation.\"\n",
        "    )\n",
        "\n",
        "    evaluator_task = (\n",
        "        \"User goal: \" + user_goal + \"\\n\\n\" +\n",
        "        \"Research summary from ResearcherAgent:\\n\" + researcher_summary\n",
        "    )\n",
        "\n",
        "    evaluator_resp = run_agent(\n",
        "        role_name=\"EvaluatorAgent\",\n",
        "        role_instructions=evaluator_instructions,\n",
        "        task=evaluator_task,\n",
        "        tools=[],  # this agent just reasons over the researcher's text\n",
        "    )\n",
        "\n",
        "    evaluator_summary = evaluator_resp.output_text\n",
        "    print(\"\\n--- EvaluatorAgent output ---\\n\")\n",
        "    print(evaluator_summary)\n",
        "\n",
        "    if hitl:\n",
        "        cont2 = input(\"\\nApprove evaluation and final recommendation? [y to accept, anything else to stop]: \")\n",
        "        if cont2.strip().lower() != \"y\":\n",
        "            print(\"\\n‚èπ Workflow stopped by human after evaluation phase.\")\n",
        "            return evaluator_summary\n",
        "\n",
        "    print(\"\\n‚úÖ Workflow complete. Returning final recommendation.\")\n",
        "    return evaluator_summary\n",
        "\n",
        "print(\"‚úÖ Multi-agent workflow function ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5122ab4b",
      "metadata": {},
      "outputs": [],
      "source": [
        "user_goal = (\n",
        "    \"I need a Hugging Face text-generation model to generate short English marketing \"\n",
        "    \"taglines for a consumer app. Latency should be low (fast inference), and the license \"\n",
        "    \"should allow commercial use.\"\n",
        ")\n",
        "\n",
        "final_recommendation = run_multi_agent_workflow(user_goal, hitl=True)\n",
        "\n",
        "print(\"\\n=== FINAL RECOMMENDATION (from workflow) ===\\n\")\n",
        "print(final_recommendation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881bcc0a",
      "metadata": {},
      "source": [
        "## üöÄ Run the multi-agent workflow\n",
        "\n",
        "Edit `user_goal` in the cell above and rerun to try different model-selection requests. With `hitl=True`, you‚Äôll be prompted to approve after the research phase and after the evaluation phase."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ddef375",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Exercises (end of phase)\n",
        "\n",
        "*Use only what you learned in this phase (workflow types, choosing workflows, personas, HITL, multi-agent code).*\n",
        "\n",
        "1. **Sequential vs parallel**  \n",
        "   For the task *\"Generate 3 tagline options for our product and then pick the best one,\"* would you choose a **sequential** or **parallel** workflow? Justify in one or two sentences (think about dependencies and diversity).\n",
        "\n",
        "2. **Where to add a human checkpoint**  \n",
        "   In our Researcher ‚Üí Evaluator workflow, the final output is a model recommendation. If that recommendation might be **shown directly to a customer**, where would you add a human-in-the-loop checkpoint (e.g. after research, after evaluation, or both)? Give a short reason.\n",
        "\n",
        "3. **Design two personas**  \n",
        "   Sketch a **2-step persona design** for: *\"Answer a support ticket: first classify the intent (billing / technical / feedback), then draft a short reply.\"* For each step give: (a) a **role name**, (b) **input** and **output**, and (c) whether that step needs a **tool** (e.g. MCP) or only the model. You can write it in a small table or bullet list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Phase 4 wrap-up\n",
        "\n",
        "In this phase you:\n",
        "- Learned common **agent workflow patterns** and how to choose among them.\n",
        "- Designed **personas and phases** for a model-selection workflow.\n",
        "- Discussed how different **frameworks** (LangChain, LlamaIndex, crewAI, AutoGen) map to these ideas.\n",
        "- Implemented a simple **multi-agent pipeline with human-in-the-loop** using Groq + MCP.\n",
        "\n",
        "You can now adapt this pattern to your own use cases: change the goal, personas, tools, or human checkpoints, or port the same design into a higher-level agent framework.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# üìù Phase 0: Encoding, Decoding & Vector Space\n",
        "\n",
        "**Before using any LLM**, we learn how text becomes **token IDs** and **vectors**. The Groq API does the same encode ‚Üí vectors ‚Üí decode internally; here you see it explicitly.\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 14px; border-radius: 8px; border-left: 4px solid #4caf50;\">\n",
        "<strong>üéØ What you'll do:</strong> Encode text ‚Üí token IDs ‚Üí show subwords ‚Üí map IDs to vectors (with a tiny demo matrix) ‚Üí decode back to text. No API key needed.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Setup** ‚Äî Install LangChain tokenizer, tiktoken, NumPy\n",
        "- **Load the tokenizer** ‚Äî LangChain + tiktoken (gpt2 encoding)\n",
        "- **1. Encode** ‚Äî Text ‚Üí token IDs (what <code>input_tokens</code> means)\n",
        "- **2. Subwords** ‚Äî What each token ID represents\n",
        "- **3. Vector space** ‚Äî Token IDs ‚Üí embedding vectors; demo matrix + cosine similarity\n",
        "- **4. Decode** ‚Äî Token IDs ‚Üí text (round-trip)\n",
        "- **5. Subword example** ‚Äî Rare word split into tokens\n",
        "- **Connection to Phase 1** ‚Äî How this maps to the LLM API\n",
        "- **Additional reading** ‚Äî Videos and blogs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Install **langchain-text-splitters**, **tiktoken**, and **numpy**. On Colab, run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain-text-splitters tiktoken numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Load the tokenizer\n",
        "\n",
        "We use **LangChain's Tokenizer** with **tiktoken** (encoding `gpt2`). Same BPE idea as many LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tiktoken\n",
        "from langchain_text_splitters import Tokenizer\n",
        "\n",
        "ENCODING_NAME = \"gpt2\"\n",
        "HIDDEN_DIM = 768\n",
        "\n",
        "enc = tiktoken.get_encoding(ENCODING_NAME)\n",
        "lc_tokenizer = Tokenizer(\n",
        "    encode=enc.encode,\n",
        "    decode=enc.decode,\n",
        "    tokens_per_chunk=1000,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "vocab_size = enc.n_vocab\n",
        "print(f\"‚úÖ Loaded tokenizer: {ENCODING_NAME} | Vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Encode: text ‚Üí token IDs\n",
        "\n",
        "The model never sees raw text. It sees **sequences of integers** (token IDs). This is what **input_tokens** means in the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The model reads and writes in tokens, not raw characters.\"\n",
        "encoded = lc_tokenizer.encode(text)\n",
        "\n",
        "print(\"üì• Encoding: text ‚Üí token IDs\")\n",
        "print(f\"   Text: {text!r}\")\n",
        "print(f\"   Token IDs: {encoded}\")\n",
        "print(f\"   Token count: {len(encoded)} ( = input_tokens in API)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ What each ID represents (subwords)\n",
        "\n",
        "Each token ID maps to a **subword** (often a word or piece of a word). Rare words get split into pieces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def id_to_token_string(e, token_id):\n",
        "    raw = e.decode_single_token_bytes(token_id)\n",
        "    return raw.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "tokens = [id_to_token_string(enc, i) for i in encoded]\n",
        "print(\"üî§ Tokens (subwords):\")\n",
        "print(f\"   {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Vector space: token IDs ‚Üí embedding vectors\n",
        "\n",
        "The model **never uses the integer ID directly**. It looks up a **vector** (embedding) for each ID. All computation happens in this vector space. Below we use a tiny **demo** embedding matrix (real models use trained weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "embedding_matrix = rng.standard_normal((vocab_size, HIDDEN_DIM)).astype(np.float32) * 0.02\n",
        "input_ids = np.array([encoded])\n",
        "embeddings = embedding_matrix[input_ids]\n",
        "seq_len, hdim = embeddings.shape[1], embeddings.shape[2]\n",
        "\n",
        "print(\"üìä Vector space (demo embedding matrix):\")\n",
        "print(f\"   Token IDs shape:  (1, {seq_len})\")\n",
        "print(f\"   Embeddings shape: (1, {seq_len}, {hdim})\")\n",
        "print(f\"   Each token ID ‚Üí one vector in a {hdim}-dim space.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "ax.bar(range(seq_len), [np.linalg.norm(embeddings[0, i]) for i in range(seq_len)], color=\"#1976d2\", alpha=0.8)\n",
        "ax.set_xlabel(\"Token position\")\n",
        "ax.set_ylabel(\"Embedding norm\")\n",
        "ax.set_title(\"üìê Norm of each token's embedding vector (demo)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity in vector space\n",
        "\n",
        "Related tokens often have **closer** embeddings (higher cosine similarity). The model \"sees\" these vectors, not the integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    a_flat = a.flatten().astype(np.float64)\n",
        "    b_flat = b.flatten().astype(np.float64)\n",
        "    return float(np.dot(a_flat, b_flat) / (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-9))\n",
        "\n",
        "idx_a, idx_b = 1, 3\n",
        "sim = cosine_similarity(embeddings[0, idx_a], embeddings[0, idx_b])\n",
        "print(f\"   Similarity (cosine): {tokens[idx_a]!r} vs {tokens[idx_b]!r} ‚Üí {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Decode: token IDs ‚Üí text\n",
        "\n",
        "Round-trip: we decode the same IDs back to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoded = lc_tokenizer.decode(encoded)\n",
        "print(\"üì§ Decoding: token IDs ‚Üí text\")\n",
        "print(f\"   Decoded: {decoded!r}\")\n",
        "print(f\"   Round-trip OK: {decoded == text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Subword example: one word ‚Üí several tokens\n",
        "\n",
        "Rare words get **split** into subword pieces. The model operates on these pieces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional reading\n",
        "\n",
        "**YouTube (verified)**  \n",
        "- [The tokenization pipeline](https://www.youtube.com/watch?v=Yffk5aydLzg) ‚Äî Hugging Face course: what happens when you call a tokenizer.  \n",
        "- [Building a new tokenizer](https://www.youtube.com/watch?v=MR8tZm5ViWU) ‚Äî Hugging Face: train and use tokenizers.\n",
        "\n",
        "**Blogs (popular)**  \n",
        "- [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) ‚Äî Hugging Face: BPE, WordPiece, SentencePiece.  \n",
        "- [tiktoken](https://github.com/openai/tiktoken) ‚Äî OpenAI: fast BPE tokenizer used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rare = \"tokenizer\"\n",
        "enc_rare = lc_tokenizer.encode(rare)\n",
        "tokens_rare = [id_to_token_string(enc, i) for i in enc_rare]\n",
        "print(f\"   Word: {rare!r} ‚Üí Token IDs: {enc_rare} ‚Üí Tokens: {tokens_rare}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó How this connects to Phase 1 (LLM API)\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 14px; border-radius: 8px; border-left: 4px solid #ff9800;\">\n",
        "<strong>In the API:</strong> Your prompt (text) ‚Üí encoded to token IDs ‚Üí each ID mapped to an embedding vector ‚Üí model runs in that vector space ‚Üí predicts next token IDs ‚Üí decoded to text. <code>input_tokens</code> / <code>output_tokens</code> = lengths of these ID sequences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Phase 0 complete. Next: Phase 1 (Groq API).\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# üìù Phase 0: Encoding, Decoding & Vector Space\n",
        "\n",
        "**Before using any LLM**, we learn how text becomes **token IDs** and **vectors**. The Groq API does the same encode ‚Üí vectors ‚Üí decode internally; here you see it explicitly.\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 14px; border-radius: 8px; border-left: 4px solid #4caf50;\">\n",
        "<strong>üéØ What you'll do:</strong> Encode text ‚Üí token IDs ‚Üí show subwords ‚Üí map IDs to vectors (with a tiny demo matrix) ‚Üí decode back to text. No API key needed.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Setup** ‚Äî Install LangChain tokenizer, tiktoken, NumPy\n",
        "- **Bag of Words (traditional)** ‚Äî Words ‚Üí vocabulary ‚Üí count vectors (before BPE)\n",
        "- **What is BPE?** ‚Äî Byte Pair Encoding: subwords, merge table, why LLMs use it\n",
        "- **Load the tokenizer** ‚Äî LangChain + tiktoken (BPE, gpt2 encoding)\n",
        "- **1. Encode** ‚Äî Text ‚Üí token IDs (what <code>input_tokens</code> means)\n",
        "- **2. Subwords** ‚Äî What each token ID represents\n",
        "- **3. Vector space** ‚Äî What vectors and vector spaces are; embeddings; dimensions; similarity\n",
        "- **4. Decode** ‚Äî Token IDs ‚Üí text (round-trip)\n",
        "- **5. Subword example** ‚Äî Rare word split into tokens\n",
        "- **Connection to Phase 1** ‚Äî How this maps to the LLM API\n",
        "- **Exercises** ‚Äî Token count vs words, rare-word subwords, BoW vs BPE\n",
        "- **Additional reading** ‚Äî Videos and blogs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Install **langchain-text-splitters**, **tiktoken**, and **numpy**. On Colab, run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q langchain-text-splitters tiktoken numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Before BPE: Bag of Words (traditional text ‚Üí vectors)\n",
        "\n",
        "**Before** subword tokenization (like Byte Pair Encoding), a simple way to turn text into vectors was **Bag of Words (BoW)**:\n",
        "\n",
        "1. **Split** text into words (e.g. by spaces).\n",
        "2. **Build a vocabulary** ‚Äî a fixed list of unique words (e.g. from a corpus).\n",
        "3. **Represent each sentence** as a vector of **word counts** (one dimension per vocabulary word).\n",
        "\n",
        "No subwords: each **word** is one unit. Unknown words are typically ignored or mapped to a special \"unknown\" index. Below we implement a minimal BoW by hand (no BPE, no tokenizer yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal Bag of Words: words ‚Üí vocabulary indices ‚Üí count vectors\n",
        "import numpy as np\n",
        "\n",
        "# Small corpus (we'll build vocabulary from this)\n",
        "sentences = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog sat on the log\",\n",
        "    \"a cat and a dog\",\n",
        "]\n",
        "\n",
        "# 1. Build vocabulary: unique words, sorted (so the same word always gets the same index)\n",
        "all_words = []\n",
        "for s in sentences:\n",
        "    all_words.extend(s.lower().split())\n",
        "vocab = sorted(set(all_words))\n",
        "word_to_id = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "print(\"üìñ Vocabulary (word ‚Üí index):\")\n",
        "print(f\"   {word_to_id}\")\n",
        "print(f\"   Size: {len(vocab)}\")\n",
        "\n",
        "# 2. Convert each sentence to a BoW vector (count of each word in the sentence)\n",
        "def bow_vector(sentence, word_to_id):\n",
        "    vec = np.zeros(len(word_to_id), dtype=np.int32)\n",
        "    for w in sentence.lower().split():\n",
        "        if w in word_to_id:\n",
        "            vec[word_to_id[w]] += 1\n",
        "    return vec\n",
        "\n",
        "print(\"\\nüìä Bag of Words vectors (one row per sentence):\")\n",
        "for s in sentences:\n",
        "    v = bow_vector(s, word_to_id)\n",
        "    print(f\"   {s!r}\")\n",
        "    print(f\"   ‚Üí {v}  (counts for: {vocab})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Limitation of BoW:** The vocabulary is **fixed** from the corpus. New or rare words (e.g. \"tokenizer\") have **no index** and are typically ignored. **Byte Pair Encoding (BPE)**, used next, splits text into **subwords** so we can represent any string with a fixed, learned set of pieces‚Äîno separate word list needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is Byte Pair Encoding (BPE)?\n",
        "\n",
        "**Byte Pair Encoding (BPE)** is a **subword** tokenization algorithm: text is split into pieces that can be smaller than a word (e.g. \"running\" ‚Üí \"run\" + \"ning\"). That way we don't need a separate entry for every word‚Äîwe learn a **fixed set of subword pieces** from data and can represent **any** string by concatenating them.\n",
        "\n",
        "**How BPE works (conceptually):**\n",
        "\n",
        "1. **Start from characters (or bytes).** The text is first split into characters (or byte-level units).\n",
        "2. **Learn merges from a corpus.** We count how often every **pair** of adjacent units appears (e.g. \"t\" + \"h\" ‚Üí \"th\"). We repeatedly **merge the most frequent pair** into a new single token and add it to the vocabulary. After many such merges we have a vocabulary of subwords (single chars, frequent chunks like \"th\", \"ing\", whole common words, etc.).\n",
        "3. **Encode new text.** To tokenize a new sentence we split it into characters, then apply the **same merge rules in order**. The result is a sequence of **token IDs** (one integer per subword).\n",
        "4. **Decode.** To go back to text we map each token ID to its subword string and concatenate.\n",
        "\n",
        "**Why LLMs use BPE:** A fixed vocabulary of ~50k subwords can represent any sentence. Rare words become several tokens (e.g. \"tokenizer\" ‚Üí \"token\" + \"izer\"); common words may stay one token. The model only ever sees **sequences of integers** (token IDs); we'll see next how each ID is then mapped to a **vector**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal BPE-style demo: merge the most frequent adjacent pair (one step)\n",
        "# In real BPE we repeat this thousands of times on a large corpus.\n",
        "\n",
        "text = \"aaabaaaba\"  # \"aa\" appears 4 times, \"ab\" 2 times, \"ba\" 2 times\n",
        "units = list(text)  # start with characters: ['a','a','a','b','a','a','a','b','a']\n",
        "\n",
        "from collections import Counter\n",
        "pairs = [(\"\".join(units[i:i+2]), i) for i in range(len(units)-1)]\n",
        "pair_counts = Counter(p for p, _ in pairs)\n",
        "most_common_pair = pair_counts.most_common(1)[0][0]  # e.g. \"aa\"\n",
        "\n",
        "print(\"üìå One BPE-style merge step (conceptual):\")\n",
        "print(f\"   Text: {text!r} ‚Üí units: {units}\")\n",
        "print(f\"   Most frequent pair: {most_common_pair!r}\")\n",
        "print(f\"   After merging that pair: we'd replace all \\\"{most_common_pair}\\\" with one new token.\")\n",
        "print(\"   Real BPE (e.g. tiktoken) does this on a huge corpus and keeps many merge rules.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Load the tokenizer\n",
        "\n",
        "We use **LangChain's Tokenizer** with **tiktoken** (encoding `gpt2`). Same BPE idea as many LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tiktoken\n",
        "from langchain_text_splitters import Tokenizer\n",
        "\n",
        "ENCODING_NAME = \"gpt2\"\n",
        "HIDDEN_DIM = 768\n",
        "\n",
        "enc = tiktoken.get_encoding(ENCODING_NAME)\n",
        "lc_tokenizer = Tokenizer(\n",
        "    encode=enc.encode,\n",
        "    decode=enc.decode,\n",
        "    tokens_per_chunk=1000,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "vocab_size = enc.n_vocab\n",
        "print(f\"‚úÖ Loaded tokenizer: {ENCODING_NAME} | Vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Encode: text ‚Üí token IDs\n",
        "\n",
        "The model never sees raw text. It sees **sequences of integers** (token IDs). This is what **input_tokens** means in the API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"The model reads and writes in tokens, not raw characters.\"\n",
        "encoded = lc_tokenizer.encode(text)\n",
        "\n",
        "print(\"üì• Encoding: text ‚Üí token IDs\")\n",
        "print(f\"   Text: {text!r}\")\n",
        "print(f\"   Token IDs: {encoded}\")\n",
        "print(f\"   Token count: {len(encoded)} ( = input_tokens in API)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ What each ID represents (subwords)\n",
        "\n",
        "Each token ID maps to a **subword** (often a word or piece of a word). Rare words get split into pieces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def id_to_token_string(e, token_id):\n",
        "    raw = e.decode_single_token_bytes(token_id)\n",
        "    return raw.decode(\"utf-8\", errors=\"replace\")\n",
        "\n",
        "tokens = [id_to_token_string(enc, i) for i in encoded]\n",
        "print(\"üî§ Tokens (subwords):\")\n",
        "print(f\"   {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Vector space: token IDs ‚Üí embedding vectors\n",
        "\n",
        "### What is a vector?\n",
        "\n",
        "A **vector** is a list of numbers, e.g. `[0.1, -0.3, 0.5, ...]`. You can think of it as a **point** in space: in 2D, two numbers give (x, y); in 3D, three numbers give (x, y, z). In NLP we use **many** numbers per token (e.g. 768)‚Äîso we can't draw it on paper, but the idea is the same: each vector is one point in a **high-dimensional space**.\n",
        "\n",
        "### What is a vector space?\n",
        "\n",
        "A **vector space** is the set of all possible vectors of a given length. For length 768 we have **R^768**: every point is a list of 768 real numbers. Distances and angles between points are well-defined (e.g. **cosine similarity** = how aligned two vectors are; **norm** = length of a vector). Models do **linear algebra** in this space: add vectors, scale them, take dot products‚Äîso turning text into vectors is what lets math do the work.\n",
        "\n",
        "### Why does the model use vectors (embeddings)?\n",
        "\n",
        "The model **never uses the token ID (integer) directly**. It looks up a **vector** (called an **embedding**) for each ID from a big table (the **embedding matrix**). All computation‚Äîattention, layers, predictions‚Äîhappens in this vector space. Similar or related tokens often have **similar embeddings** (high cosine similarity); the model was trained so that meaning is reflected in geometry. Below we use a tiny **demo** embedding matrix (random); real models use **trained** weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "embedding_matrix = rng.standard_normal((vocab_size, HIDDEN_DIM)).astype(np.float32) * 0.02\n",
        "input_ids = np.array([encoded])\n",
        "embeddings = embedding_matrix[input_ids]\n",
        "seq_len, hdim = embeddings.shape[1], embeddings.shape[2]\n",
        "\n",
        "print(\"üìä Vector space (demo embedding matrix):\")\n",
        "print(f\"   Token IDs shape:  (1, {seq_len})\")\n",
        "print(f\"   Embeddings shape: (1, {seq_len}, {hdim})\")\n",
        "print(f\"   Each token ID ‚Üí one vector in a {hdim}-dim space.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 3))\n",
        "ax.bar(range(seq_len), [np.linalg.norm(embeddings[0, i]) for i in range(seq_len)], color=\"#1976d2\", alpha=0.8)\n",
        "ax.set_xlabel(\"Token position\")\n",
        "ax.set_ylabel(\"Embedding norm\")\n",
        "ax.set_title(\"üìê Norm of each token's embedding vector (demo)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensions and the norm (length)\n",
        "\n",
        "Each token's embedding has **768 numbers**‚Äîso we're in a **768-dimensional** space. We can't draw that, but we can still measure things: the **norm** (length) of a vector is like the length of an arrow. The bar chart above shows the norm of each token's embedding (our demo matrix is random; in trained models these lengths and directions carry meaning).\n",
        "\n",
        "### Similarity in vector space\n",
        "\n",
        "**Cosine similarity** measures how much two vectors point in the same direction: **1** = same direction, **0** = perpendicular, **-1** = opposite. In trained models, tokens with similar meaning often have **high cosine similarity** (their vectors point the same way). The model \"sees\" only these vectors‚Äînot the token IDs‚Äîand uses distances and angles to reason. Below we compute cosine similarity between two token embeddings (with our demo matrix the result is random; in a real model you'd see related words cluster)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "    a_flat = a.flatten().astype(np.float64)\n",
        "    b_flat = b.flatten().astype(np.float64)\n",
        "    return float(np.dot(a_flat, b_flat) / (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-9))\n",
        "\n",
        "idx_a, idx_b = 1, 3\n",
        "sim = cosine_similarity(embeddings[0, idx_a], embeddings[0, idx_b])\n",
        "print(f\"   Similarity (cosine): {tokens[idx_a]!r} vs {tokens[idx_b]!r} ‚Üí {sim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Decode: token IDs ‚Üí text\n",
        "\n",
        "Round-trip: we decode the same IDs back to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoded = lc_tokenizer.decode(encoded)\n",
        "print(\"üì§ Decoding: token IDs ‚Üí text\")\n",
        "print(f\"   Decoded: {decoded!r}\")\n",
        "print(f\"   Round-trip OK: {decoded == text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Subword example: one word ‚Üí several tokens\n",
        "\n",
        "Rare words get **split** into subword pieces. The model operates on these pieces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Exercises\n",
        "\n",
        "*Use only what you learned in this phase (encoding, decoding, subwords, vector space).*\n",
        "\n",
        "1. **Token count vs word count**  \n",
        "   Encode the sentence *\"The tokenizer splits text into subwords.\"* with the same tokenizer you used above. Why might the number of token IDs be different from the number of words? Give a short explanation.\n",
        "\n",
        "2. **Rare word and subwords**  \n",
        "   Pick a rare or technical word (e.g. *\"tokenizer\"*, *\"BPE\"*, or *\"embedding\"*). Encode it and list the subword pieces. In one or two sentences, explain why the tokenizer might split it that way (e.g. why it might be one token vs several).\n",
        "\n",
        "3. **Similar meaning, different words**  \n",
        "   Consider two sentences: *\"The cat sat on the mat.\"* and *\"A feline rested on the rug.\"* They have similar meaning but different words. If you used **Bag of Words** (word counts only), would the two vectors be similar? What if you used **BPE token IDs**? Explain briefly why BoW and token IDs behave differently here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional reading\n",
        "\n",
        "**YouTube (verified)**  \n",
        "- [The tokenization pipeline](https://www.youtube.com/watch?v=Yffk5aydLzg) ‚Äî Hugging Face course: what happens when you call a tokenizer.  \n",
        "- [Building a new tokenizer](https://www.youtube.com/watch?v=MR8tZm5ViWU) ‚Äî Hugging Face: train and use tokenizers.\n",
        "\n",
        "**Blogs (popular)**  \n",
        "- [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) ‚Äî Hugging Face: BPE, WordPiece, SentencePiece.  \n",
        "- [tiktoken](https://github.com/openai/tiktoken) ‚Äî OpenAI: fast BPE tokenizer used in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rare = \"tokenizer\"\n",
        "enc_rare = lc_tokenizer.encode(rare)\n",
        "tokens_rare = [id_to_token_string(enc, i) for i in enc_rare]\n",
        "print(f\"   Word: {rare!r} ‚Üí Token IDs: {enc_rare} ‚Üí Tokens: {tokens_rare}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîó How this connects to Phase 1 (LLM API)\n",
        "\n",
        "<div style=\"background: #fff3e0; padding: 14px; border-radius: 8px; border-left: 4px solid #ff9800;\">\n",
        "<strong>In the API:</strong> Your prompt (text) ‚Üí encoded to token IDs ‚Üí each ID mapped to an embedding vector ‚Üí model runs in that vector space ‚Üí predicts next token IDs ‚Üí decoded to text. <code>input_tokens</code> / <code>output_tokens</code> = lengths of these ID sequences.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Phase 0 complete. Next: Phase 1 (Groq API).\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

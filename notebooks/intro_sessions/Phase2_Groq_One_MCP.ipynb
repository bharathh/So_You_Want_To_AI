{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# üîå Phase 2: Groq + One MCP (Hugging Face)\n",
        "\n",
        "Add **one MCP server** so the model can use **tools** (e.g. search Hugging Face). You'll learn what MCP is, why it's needed, how to choose and find MCP servers, and end with a **real-world application**.\n",
        "\n",
        "<div style=\"background: #fce4ec; padding: 14px; border-radius: 8px; border-left: 4px solid #c2185b;\">\n",
        "<strong>üéØ What you'll do:</strong> Understand MCP servers (need, evaluation, examples, where to find them), call Groq with one MCP tool, and build a practical MCP-based workflow.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Setup** ‚Äî OpenAI client, API keys (GROQ, optional HF)\n",
        "- **What is MCP?** ‚Äî Model Context Protocol; servers, tools, resources\n",
        "- **Why do we need MCP?** ‚Äî LLMs can't access tools/data natively; MCP standardizes it\n",
        "- **What can you do with an MCP server?** ‚Äî Tools, resources, prompts\n",
        "- **How to evaluate what MCP servers you need** ‚Äî Task ‚Üí capabilities ‚Üí pick server\n",
        "- **Examples with MCP** ‚Äî Search, read, call APIs; Hugging Face example\n",
        "- **How to find MCP servers** ‚Äî Registry, GitHub, docs, Groq/Cursor\n",
        "- **Define the MCP tool** ‚Äî type, server URL, require_approval: \"never\"\n",
        "- **Call Groq with the MCP tool** ‚Äî One request; model uses tool and returns text\n",
        "- **Output items** ‚Äî Types in <code>response.output</code> (e.g. mcp_call)\n",
        "- **Real-world application** ‚Äî Practical MCP workflow (e.g. model discovery + summary)\n",
        "- **Exercises** ‚Äî Why MCP, matching task to server, changing the real-world prompt\n",
        "- **Additional reading** ‚Äî MCP spec and Groq docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Install **openai**. On Colab, run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîë API keys\n",
        "\n",
        "- **GROQ_API_KEY** ‚Äî required. Get it at [console.groq.com](https://console.groq.com/keys).<br>\n",
        "- **HF_TOKEN** (optional) ‚Äî for Hugging Face MCP; improves rate limits. Get it at [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Paste your GROQ_API_KEY: \")\n",
        "if not os.environ.get(\"HF_TOKEN\"):\n",
        "    tok = getpass(\"Paste your HF_TOKEN (or press Enter to skip): \")\n",
        "    if tok:\n",
        "        os.environ[\"HF_TOKEN\"] = tok\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_groq_client():\n",
        "    return OpenAI(\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "    )\n",
        "\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "print(\"‚úÖ Groq client ready.\" if os.environ.get(\"GROQ_API_KEY\") else \"Need GROQ_API_KEY.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is the Model Context Protocol (MCP)?\n",
        "\n",
        "**MCP** is an **open protocol** that lets LLM applications (like Groq, Cursor, or Claude) talk to **external tools and data** in a standard way.\n",
        "\n",
        "- **MCP server** ‚Äî A service that exposes **tools** (actions the model can call, e.g. ‚Äúsearch Hugging Face‚Äù), **resources** (read-only data, e.g. files or APIs), and optionally **prompts** (predefined prompt templates). The server runs somewhere (your machine, a cloud URL) and speaks the MCP protocol (JSON-RPC over stdio or HTTP/SSE).\n",
        "- **MCP client** ‚Äî The application that has the model (e.g. Groq API, Cursor). It connects to one or more MCP servers, sends the user‚Äôs prompt, and when the model decides to use a tool, the client calls that tool on the server and returns the result to the model.\n",
        "- **Tool** ‚Äî One capability the server provides (e.g. ‚Äúsearch_models‚Äù, ‚Äúread_file‚Äù). The model sees tool names and descriptions, chooses when to call them, and receives structured results.\n",
        "\n",
        "So: **you add an MCP server** (by URL or config) to your client; the **model then can use that server‚Äôs tools** during a conversation, without you hand-writing glue code for each integration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why do we need MCP?\n",
        "\n",
        "LLMs only see **text**. They can‚Äôt open files, call APIs, or search the web by themselves. To do real tasks (search models, read docs, run code), something has to:\n",
        "\n",
        "1. **Expose** those actions as callable ‚Äútools‚Äù with clear names and parameters.\n",
        "2. **Run** the tool when the model asks and **return** the result as text (or structured data) back to the model.\n",
        "3. Do this in a **consistent way** across many apps and providers.\n",
        "\n",
        "**Without a standard:** Every app (Cursor, Groq, Claude, etc.) would invent its own way to plug in tools ‚Üí more work for developers and fewer reusable integrations.\n",
        "\n",
        "**With MCP:** One protocol for ‚Äúwhat tools exist‚Äù and ‚Äúcall this tool with these arguments.‚Äù Servers (Hugging Face, filesystem, Slack, etc.) implement MCP once; any MCP-capable client can use them. So you get **one way to add capabilities** (search, read, execute) to any model that supports MCP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What can you do with an MCP server?\n",
        "\n",
        "MCP servers expose three kinds of things (depending on the server):\n",
        "\n",
        "| Capability | What it is | Example |\n",
        "|------------|------------|---------|\n",
        "| **Tools** | Actions the model can invoke (with arguments). The server runs the action and returns a result. | Search Hugging Face, run a script, create a calendar event, query a database. |\n",
        "| **Resources** | Read-only data the model can read (by URI). | Read a file, fetch a URL, list directory contents. |\n",
        "| **Prompts** | Predefined prompt templates the client can offer (e.g. ‚ÄúSummarize this doc‚Äù). | ‚ÄúCode review‚Äù, ‚ÄúExplain this error‚Äù, ‚ÄúTurn notes into a ticket‚Äù. |\n",
        "\n",
        "In this notebook we use **tools** only (e.g. Hugging Face search). The model sees the tool list, chooses when to call one, and the Groq API sends that call to the MCP server and feeds the result back to the model so it can continue the reply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to evaluate what MCP servers you need\n",
        "\n",
        "1. **Define the task** ‚Äî What should the model do? (e.g. ‚Äúfind a model on Hugging Face‚Äù, ‚Äúread my README‚Äù, ‚Äúsearch the web‚Äù, ‚Äúrun a linter‚Äù.)\n",
        "2. **Map to capabilities** ‚Äî Does the task need **search**, **read a file**, **call an API**, **run code**, **access a database**, etc.?\n",
        "3. **Match to servers** ‚Äî Look for MCP servers that expose those capabilities (e.g. Hugging Face MCP for model search, filesystem MCP for local files, custom server for your API).\n",
        "4. **Check auth and environment** ‚Äî Does the server need an API key (e.g. HF_TOKEN), or run only locally? Groq can call servers that are reachable by URL (e.g. https://huggingface.co/mcp).\n",
        "5. **Start minimal** ‚Äî Add one server, test with a few prompts, then add more servers if you need more capabilities.\n",
        "\n",
        "**Example:** ‚ÄúI want the model to recommend a Hugging Face model for sentiment analysis‚Äù ‚Üí need **search/list models** ‚Üí use **Hugging Face MCP** (exposes model search). No need for filesystem or database MCP unless the task also involves local files or DB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Examples of things to do with an MCP server\n",
        "\n",
        "| Goal | MCP server (example) | What the model can do |\n",
        "|------|----------------------|------------------------|\n",
        "| **Find / compare models** | Hugging Face MCP | Search models, read model cards, list trending or task-specific models. |\n",
        "| **Read local files** | Filesystem MCP | Read files, list dirs; e.g. ‚Äúsummarize this README‚Äù, ‚Äúwhat‚Äôs in this folder?‚Äù |\n",
        "| **Search the web** | Web search MCP (if available) | Search and get snippets; e.g. ‚Äúlatest docs for library X‚Äù. |\n",
        "| **Use your API** | Custom MCP server | Expose your API as tools; model calls endpoints and uses responses in the reply. |\n",
        "| **Databases** | DB MCP (e.g. Postgres) | Query (read-only or controlled writes); e.g. ‚Äúhow many users signed up this week?‚Äù |\n",
        "| **Slack / Notion / Google** | Official or community MCPs | Post messages, read/write docs, create calendar events from natural language. |\n",
        "\n",
        "In this notebook we use **Hugging Face MCP**: the model can search models, read cards, and answer questions like ‚ÄúWhat models are trending?‚Äù or ‚ÄúSuggest a model for translation.‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to find MCP servers\n",
        "\n",
        "- **Official / vendor registries** ‚Äî [modelcontextprotocol.io](https://modelcontextprotocol.io) and Anthropic‚Äôs MCP docs list servers (e.g. Google Drive, Slack, GitHub). Groq and Cursor docs also mention supported or recommended MCPs.\n",
        "- **GitHub** ‚Äî Search for ‚ÄúMCP server‚Äù or ‚Äúmodel context protocol‚Äù; many repos implement servers for Hugging Face, filesystem, Postgres, custom APIs, etc. Check the README for the server URL or how to run it.\n",
        "- **Hugging Face** ‚Äî The Hugging Face MCP we use here is documented at [huggingface.co/mcp](https://huggingface.co/mcp); you use the URL `https://huggingface.co/mcp` and optionally pass `HF_TOKEN` for higher rate limits.\n",
        "- **Build your own** ‚Äî Implement the MCP spec (tools/resources/prompts) in any language; expose it over HTTP/SSE or stdio so the client can connect. Useful for internal APIs or custom data sources.\n",
        "\n",
        "**Tip:** Prefer servers that document their **tools** (names, arguments, return shape) and **auth** (API key, OAuth, or none) so you know what the model can do and how to configure it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Define the MCP tool\n",
        "\n",
        "One tool entry: <code>type: \"mcp\"</code>, server URL, and <code>require_approval: \"never\"</code> so the run doesn't wait for approval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mcp_tool = {\n",
        "    \"type\": \"mcp\",\n",
        "    \"server_label\": \"Huggingface\",\n",
        "    \"server_url\": \"https://huggingface.co/mcp\",\n",
        "    \"server_description\": \"Search and access AI models from Hugging Face\",\n",
        "    \"require_approval\": \"never\",\n",
        "}\n",
        "if HF_TOKEN:\n",
        "    mcp_tool[\"headers\"] = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "print(\"MCP tool: Hugging Face (require_approval: never)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Call Groq with the MCP tool\n",
        "\n",
        "We use a model that supports tools (e.g. <code>openai/gpt-oss-120b</code>). The model may issue **tool calls**; with <code>require_approval: \"never\"</code> they run automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = get_groq_client()\n",
        "\n",
        "response = client.responses.create(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    input=\"What models are trending on Hugging Face? List up to 3.\",\n",
        "    tools=[mcp_tool],\n",
        ")\n",
        "\n",
        "print(\"--- output_text ---\")\n",
        "print(response.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Output items (types)\n",
        "\n",
        "The <code>response.output</code> list can contain text and tool-related items (e.g. <code>mcp_call</code>)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"--- output item types ---\")\n",
        "for item in response.output:\n",
        "    t = getattr(item, \"type\", item)\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real-world application: model discovery and recommendation\n",
        "\n",
        "**Use case:** A developer wants to pick a **Hugging Face model for a task** (e.g. sentiment analysis or translation) without browsing the Hub manually. The model uses the **Hugging Face MCP** to search and read model info, then returns a short recommendation.\n",
        "\n",
        "**Flow:** You ask in natural language (e.g. ‚ÄúFind a good model for sentiment analysis in English and summarize why it‚Äôs a good fit‚Äù). The LLM:\n",
        "\n",
        "1. Calls the Hugging Face MCP **tool(s)** to search or list models (e.g. by task or name).\n",
        "2. Receives tool results (model names, cards, metrics).\n",
        "3. Synthesizes a short recommendation and explanation in plain language.\n",
        "\n",
        "Below we run one such request: the model uses the MCP server to answer a practical ‚Äúfind and recommend‚Äù question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Real-world application: use MCP to find and recommend a model for a task\n",
        "client = get_groq_client()\n",
        "\n",
        "# Same MCP tool as before (Hugging Face)\n",
        "mcp_tool = {\n",
        "    \"type\": \"mcp\",\n",
        "    \"server_label\": \"Huggingface\",\n",
        "    \"server_url\": \"https://huggingface.co/mcp\",\n",
        "    \"server_description\": \"Search and access AI models from Hugging Face\",\n",
        "    \"require_approval\": \"never\",\n",
        "}\n",
        "if os.environ.get(\"HF_TOKEN\"):\n",
        "    mcp_tool[\"headers\"] = {\"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\"}\n",
        "\n",
        "# Practical prompt: find a model for a task and summarize why it fits\n",
        "prompt = (\n",
        "    \"I need a model for sentiment analysis on English product reviews. \"\n",
        "    \"Use Hugging Face to find a suitable model, then in one short paragraph \"\n",
        "    \"recommend one and explain why it's a good fit (e.g. task, license, popularity).\"\n",
        ")\n",
        "\n",
        "resp = client.responses.create(\n",
        "    model=\"openai/gpt-oss-120b\",\n",
        "    input=prompt,\n",
        "    tools=[mcp_tool],\n",
        ")\n",
        "\n",
        "print(\"--- Real-world application: model recommendation ---\")\n",
        "print(resp.output_text)\n",
        "print(\"\\n--- Tool calls used (mcp_call items) ---\")\n",
        "for item in resp.output:\n",
        "    if getattr(item, \"type\", None) == \"mcp_call\":\n",
        "        print(f\"  Tool: {getattr(item, 'name', '?')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Phase 2 complete. Next: Phase 3 (multi-step MCP agent).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Exercises\n",
        "\n",
        "*Use only what you learned in this phase (MCP, tools, server choice, Groq + MCP call, mcp_call).*\n",
        "\n",
        "1. **Why MCP?**  \n",
        "   In one sentence, why can't an LLM \"search Hugging Face\" by itself‚Äîi.e. without an MCP server or similar‚Äîand what does the MCP server provide?\n",
        "\n",
        "2. **Matching task to server**  \n",
        "   Your task is: *\"Read a local README file and summarize it in three bullet points.\"* What **capability** do you need (e.g. read file, search web, call API)? What **kind** of MCP server would you look for, and what would it expose (tools, resources, or both)?\n",
        "\n",
        "3. **Change the real-world prompt**  \n",
        "   In the \"Real-world application\" section, change the prompt so the user asks for a Hugging Face model for **image classification** (instead of sentiment analysis). Run the cell and compare: does the model use the MCP tool? What models does it recommend? Write one sentence on what you observed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional reading\n",
        "\n",
        "**YouTube (verified)**  \n",
        "- [The Model Context Protocol (MCP)](https://www.youtube.com/watch?v=CQywdSdi5iA) ‚Äî Anthropic: MCP intro and why it matters.  \n",
        "- [Model Context Protocol, clearly explained](https://www.youtube.com/watch?v=7j_NE6Pjv-E) ‚Äî Why MCP matters for connecting AI to tools.\n",
        "\n",
        "**Blogs (popular)**  \n",
        "- [Model Context Protocol ‚Äì Specification](https://modelcontextprotocol.io/specification/latest) ‚Äî Official MCP spec: tools, resources, lifecycle.  \n",
        "- [Groq API Reference](https://console.groq.com/docs) ‚Äî Groq docs: Responses API, tools, MCP connectors."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

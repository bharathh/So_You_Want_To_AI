{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# ü§ñ Phase 1: One Groq API Call (No MCP)\n",
        "\n",
        "Learn **prompt engineering** basics, how **parameters** change the model's output, and how to add **context** to your prompts. Same encode ‚Üí vectors ‚Üí decode as Phase 0 happens on the server; here we focus on **prompts**, **temperature**, **top_p**, **max_output_tokens**, and **instructions**.\n",
        "\n",
        "<div style=\"background: #e3f2fd; padding: 14px; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
        "<strong>üéØ What you'll do:</strong> Prompt basics, parameter tuning (how it changes output), common activities for the right settings, and how to include more context in your prompt.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Setup** ‚Äî Install OpenAI client (Groq-compatible), API key\n",
        "- **Basics of prompt engineering** ‚Äî What is a prompt; input vs instructions; clarity, role, task, format\n",
        "- **Parameters** ‚Äî temperature, top_p, max_output_tokens, truncation, instructions\n",
        "- **How tuning parameters changes output** ‚Äî Temperature, length, instructions (with examples)\n",
        "- **API call** ‚Äî Single Groq Responses API request\n",
        "- **Token usage** ‚Äî input_tokens, output_tokens (with a simple chart)\n",
        "- **Output text** ‚Äî Model reply\n",
        "- **Common activities for the right settings** ‚Äî Checklist: factual vs creative, length, instructions, iteration\n",
        "- **Including more context in your prompt** ‚Äî Longer input, few-shot examples, structure, instructions\n",
        "- **Try it yourself** ‚Äî Suggestions to tweak prompts and parameters\n",
        "- **Exercises** ‚Äî Factual answers, structured prompts, shortening replies\n",
        "- **Additional reading** ‚Äî Videos and blogs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Install **openai** (Groq is OpenAI-compatible). On Colab, run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -q openai matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîë Set your Groq API key\n",
        "\n",
        "Get a free key at [console.groq.com](https://console.groq.com/keys). In Colab you can use **Secrets** or run the cell below and paste when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Groq client ready.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Paste your GROQ_API_KEY: \")\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_groq_client():\n",
        "    return OpenAI(\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Groq client ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è Basics of prompt engineering\n",
        "\n",
        "A **prompt** is the text you send to the model. Good prompts are **clear**, **specific**, and give the model a **role**, **task**, and (optionally) **format**.\n",
        "\n",
        "### Input vs instructions\n",
        "\n",
        "- **`input`** ‚Äî The main user message (the question or request). This is what the model ‚Äúsees‚Äù as the current turn.\n",
        "- **`instructions`** ‚Äî Optional **system** message: tone, role, or global rules (e.g. ‚ÄúYou are a helpful assistant. Answer in one short sentence.‚Äù). The model treats this as background context for the whole conversation.\n",
        "\n",
        "Use **instructions** for *how* the model should behave (role, tone, length). Use **input** for *what* you‚Äôre asking (the actual question or task).\n",
        "\n",
        "### What makes a good prompt?\n",
        "\n",
        "1. **Role** ‚Äî ‚ÄúYou are a Python tutor‚Äù / ‚ÄúYou are a summarizer‚Äù so the model knows the style.\n",
        "2. **Task** ‚Äî Say exactly what you want: ‚ÄúSummarize the following in 2 sentences‚Äù vs ‚ÄúSummarize this.‚Äù\n",
        "3. **Format** ‚Äî ‚ÄúReply with a bullet list‚Äù / ‚ÄúOne sentence only‚Äù / ‚ÄúJSON with keys: title, summary.‚Äù\n",
        "4. **Context** ‚Äî Put relevant facts, documents, or examples *in* the prompt (or in instructions) so the model has something to work with.\n",
        "\n",
        "Below we‚Äôll use a simple **input** and optional **instructions**; later we‚Äôll add more context and structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vague prompt: 'Tell me about MCP'\n",
            "Clear prompt: 'In one sentence, what is the Model Context Protocol? Explain for a developer.'\n",
            "\n",
            "Clear prompt specifies: task (one sentence), topic (MCP), audience (developer).\n"
          ]
        }
      ],
      "source": [
        "# Example: same task with a vague vs a clear prompt (we'll call the API with the clear one later)\n",
        "vague = \"Tell me about MCP\"\n",
        "clear = \"In one sentence, what is the Model Context Protocol? Explain for a developer.\"\n",
        "\n",
        "print(\"Vague prompt:\", repr(vague))\n",
        "print(\"Clear prompt:\", repr(clear))\n",
        "print(\"\\nClear prompt specifies: task (one sentence), topic (MCP), audience (developer).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è Parameters you can change\n",
        "\n",
        "<div style=\"background: #fff8e1; padding: 12px; border-radius: 8px;\">\n",
        "<strong>Sampling:</strong> <code>temperature</code> (0 = deterministic, 2 = very random), <code>top_p</code> (nucleus sampling).<br>\n",
        "<strong>Length:</strong> <code>max_output_tokens</code> caps the reply length.<br>\n",
        "<strong>Context:</strong> <code>truncation</code> = \"auto\" trims long inputs; \"disabled\" = no trim.<br>\n",
        "<strong>Behavior:</strong> <code>instructions</code> (system) fixes tone or task.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How tuning parameters changes the output\n",
        "\n",
        "| Parameter | Low / strict | Effect | High / loose | Effect |\n",
        "|-----------|----------------|--------|----------------|--------|\n",
        "| **temperature** | 0 | Same prompt ‚Üí same reply (deterministic). Best for facts, code, exact answers. | 0.8‚Äì1.2 | More variety, creativity; may be less consistent. Best for brainstorming, varied phrasing. |\n",
        "| **top_p** | 0.1 | Only the most likely tokens (narrow). | 1.0 | No nucleus cutoff; use with temperature for diversity. |\n",
        "| **max_output_tokens** | 50 | Short replies; good for one sentence or a list. | 500+ | Longer replies; risk of rambling if the task is vague. |\n",
        "| **instructions** | \"One sentence only.\" | Constrains style and length. | None | Model chooses length and style. |\n",
        "\n",
        "**Practical rule of thumb:** Use **low temperature (0‚Äì0.3)** for factual, reproducible answers; **higher (0.7‚Äì1.0)** for creative or varied text. Set **max_output_tokens** to the length you need (e.g. 100 for a short summary). Use **instructions** to fix role, tone, and format so you don‚Äôt rely only on the user prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Temperature 0 (deterministic) ---\n",
            "The Model Context Protocol is a proposed standard for describing and exchanging information about machine learning models, including their training data, performance metrics, and other relevant context, to facilitate transparency, explainability, and reproducibility.\n",
            "\n",
            "--- Temperature 2 (more random) ---\n",
            "The Model Context Protocol is a proposed standard for representing, communicating, and sharing modeling contexts, which facilitates collaboration, comparison, and re-use of conceptual models from multiple disciplines and organizations.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Compare output with low vs high temperature (same prompt)\n",
        "client = get_groq_client()\n",
        "prompt = \"In one sentence, what is the Model Context Protocol?\"\n",
        "\n",
        "for temp, label in [(0.0, \"Temperature 0 (deterministic)\"), (2, \"Temperature 2 (more random)\")]:\n",
        "    r = client.responses.create(\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        input=prompt,\n",
        "        temperature=temp,\n",
        "        max_output_tokens=80,\n",
        "    )\n",
        "    print(f\"--- {label} ---\")\n",
        "    print(r.output_text)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = get_groq_client()\n",
        "\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 1.0\n",
        "MAX_OUTPUT_TOKENS = 150\n",
        "TRUNCATION = \"disabled\"\n",
        "INSTRUCTIONS = None\n",
        "\n",
        "kwargs = {\n",
        "    \"model\": \"llama-3.3-70b-versatile\",\n",
        "    \"input\": \"In one sentence, what is the Model Context Protocol?\",\n",
        "    \"temperature\": TEMPERATURE,\n",
        "    \"top_p\": TOP_P,\n",
        "    \"max_output_tokens\": MAX_OUTPUT_TOKENS,\n",
        "    \"truncation\": TRUNCATION,\n",
        "}\n",
        "if INSTRUCTIONS is not None:\n",
        "    kwargs[\"instructions\"] = INSTRUCTIONS\n",
        "\n",
        "response = client.responses.create(**kwargs)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Token usage (tokenizing)\n",
        "\n",
        "Input and output are counted in **tokens**, not characters‚Äîsame idea as Phase 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usage = getattr(response, \"usage\", None)\n",
        "if usage:\n",
        "    inp = getattr(usage, \"input_tokens\", 0)\n",
        "    out = getattr(usage, \"output_tokens\", 0)\n",
        "    tot = getattr(usage, \"total_tokens\", inp + out)\n",
        "    print(\"üìà Token usage:\")\n",
        "    print(f\"   input_tokens:  {inp}\")\n",
        "    print(f\"   output_tokens: {out}\")\n",
        "    print(f\"   total_tokens:  {tot}\")\n",
        "else:\n",
        "    print(\"Usage not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if usage:\n",
        "    inp = getattr(usage, \"input_tokens\", 0)\n",
        "    out = getattr(usage, \"output_tokens\", 0)\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.bar([\"Input\", \"Output\"], [inp, out], color=[\"#1976d2\", \"#388e3c\"], alpha=0.8)\n",
        "    ax.set_ylabel(\"Tokens\")\n",
        "    ax.set_title(\"üìä Input vs output tokens\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Comparing models: size & specialization\n",
        "\n",
        "Different models can give different answers to the **same prompt**:\n",
        "\n",
        "- **Smaller / lower-parameter models** (often called *instant* / *fast*):\n",
        "  - Faster, cheaper, great for simple tasks, drafts, or high-volume workloads.\n",
        "  - May struggle more with complex reasoning or following subtle instructions.\n",
        "- **Larger models** (more parameters, e.g. 70B):\n",
        "  - Better at nuanced reasoning, complex instructions, and edge cases.\n",
        "  - Slower and more expensive per token.\n",
        "- **Specialized / tuned models (e.g. coding-tuned)**:\n",
        "  - Trained or tuned specifically for code, chat, or other domains.\n",
        "  - Often better at formatting, idiomatic style, and domain-specific tasks.\n",
        "\n",
        "**Prompt differences between models:**\n",
        "- Smaller or older models often need **more explicit instructions** (role, format, constraints).\n",
        "- Code-tuned models may follow code-style instructions better (e.g. ‚ÄúPEP8-compliant‚Äù, ‚Äúadd docstring‚Äù, ‚Äúreturn only code‚Äù).\n",
        "- Safety / refusal behavior may differ; sometimes you need to be clearer about what is allowed or provide more benign context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response.output_text)\n",
        "print(\"\\n--- raw output (first 400 chars) ---\")\n",
        "print(str(response.output)[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Common activities to get the right settings\n",
        "\n",
        "Use this as a short checklist when tuning your call:\n",
        "\n",
        "| Goal | What to do |\n",
        "|------|------------|\n",
        "| **Factual, reproducible answers** | Set **temperature** to 0 (or &lt; 0.3). Same prompt ‚Üí same reply. |\n",
        "| **Creative or varied text** | Use **temperature** 0.7‚Äì1.0. Try the same prompt twice to see variation. |\n",
        "| **Short replies** | Set **max_output_tokens** (e.g. 50‚Äì100). Add **instructions** like ‚ÄúOne sentence only.‚Äù |\n",
        "| **Longer replies** | Increase **max_output_tokens** (e.g. 300‚Äì500). Be specific in the prompt so the model doesn‚Äôt ramble. |\n",
        "| **Fixed tone or role** | Put it in **instructions** (e.g. ‚ÄúYou are a concise technical writer.‚Äù). |\n",
        "| **Stable format** | Ask in **input** or **instructions**: ‚ÄúReply with a bullet list‚Äù / ‚ÄúJSON with keys: ‚Ä¶‚Äù |\n",
        "| **Check cost/length** | Read **usage** (input_tokens, output_tokens). Short prompts + low max_output_tokens = fewer tokens. |\n",
        "| **Iterate** | If the output is wrong or noisy: clarify the prompt, add an example, or tighten instructions; then re-run. |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare different models on a small coding task\n",
        "client = get_groq_client()\n",
        "\n",
        "# Replace the second entry with a smaller or code-tuned model you have access to.\n",
        "MODELS_TO_COMPARE = [\n",
        "    (\"llama-3.3-70b-versatile\", \"General 70B (versatile)\"),\n",
        "    (\"llama-3.1-8b-instant\", \"Smaller / faster model (example)\")\n",
        "]\n",
        "\n",
        "coding_prompt = (\n",
        "    \"Write a short Python function `is_valid_ipv4` that returns True if a string \"\n",
        "    \"is a valid IPv4 address, otherwise False. Include a one-line docstring.\"\n",
        ")\n",
        "\n",
        "for model_id, label in MODELS_TO_COMPARE:\n",
        "    print(f\"--- {label} ({model_id}) ---\")\n",
        "    try:\n",
        "        r = client.responses.create(\n",
        "            model=model_id,\n",
        "            input=coding_prompt,\n",
        "            temperature=0,\n",
        "            max_output_tokens=200,\n",
        "        )\n",
        "        print(r.output_text.strip())\n",
        "    except Exception as e:\n",
        "        print(\"Error calling model:\", e)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìé Including more context in your prompt\n",
        "\n",
        "The model only ‚Äúsees‚Äù what you send. More **relevant context** in the prompt usually improves answers.\n",
        "\n",
        "### 1. Put context in `input`\n",
        "\n",
        "- **Longer input** ‚Äî Paste the document, article, or code you want summarized or questioned. The model uses it as context (subject to context-window limits).\n",
        "- **Structure it** ‚Äî Use headings, bullets, or labels so the model can find the right part: e.g. ‚ÄúContext: ‚Ä¶‚Äù then ‚ÄúQuestion: ‚Ä¶‚Äù.\n",
        "\n",
        "### 2. Use `instructions` for role and rules\n",
        "\n",
        "- **Role** ‚Äî ‚ÄúYou are a Python expert.‚Äù / ‚ÄúYou are a summarizer for non-experts.‚Äù\n",
        "- **Rules** ‚Äî ‚ÄúAlways answer in one short paragraph.‚Äù / ‚ÄúIf unsure, say so.‚Äù\n",
        "- **Format** ‚Äî ‚ÄúReply with a bullet list.‚Äù / ‚ÄúOutput valid JSON only.‚Äù\n",
        "\n",
        "### 3. Few-shot examples (in the prompt)\n",
        "\n",
        "- Give 1‚Äì3 **example input ‚Üí output** pairs in the **input** text. The model will tend to follow the same format or style.\n",
        "- Example: ‚ÄúExample 1: ‚Ä¶ ‚Üí Summary: ‚Ä¶ Example 2: ‚Ä¶ ‚Üí Summary: ‚Ä¶ Now summarize: [your text]‚Äù\n",
        "\n",
        "### 4. Truncation for long context\n",
        "\n",
        "- If your **input** is very long, set **truncation** to `\"auto\"` so the API can trim it to fit the model‚Äôs context window. Otherwise the request may fail or the model may miss the end.\n",
        "\n",
        "Below: an example with **instructions** (role + length) and a **structured input** (context + question)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: more context ‚Äî instructions (role + format) + structured input (context + question)\n",
        "client = get_groq_client()\n",
        "\n",
        "instructions = (\n",
        "    \"You are a helpful assistant for developers. \"\n",
        "    \"Answer in 1-2 short sentences. Be precise.\"\n",
        ")\n",
        "context = (\n",
        "    \"Context: The Model Context Protocol (MCP) is an open protocol that lets \"\n",
        "    \"LLM applications connect to external tools and data sources in a standard way.\\n\\n\"\n",
        "    \"Question: What is MCP and why would a developer use it?\"\n",
        ")\n",
        "\n",
        "r = client.responses.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    input=context,\n",
        "    instructions=instructions,\n",
        "    temperature=0.3,\n",
        "    max_output_tokens=80,\n",
        ")\n",
        "print(\"Instructions (system):\", instructions[:60] + \"...\")\n",
        "print(\"\\nInput (structured):\", context[:100] + \"...\")\n",
        "print(\"\\n--- Model reply ---\")\n",
        "print(r.output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Try it yourself\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 12px; border-radius: 8px; border-left: 4px solid #4caf50;\">\n",
        "<strong>Prompts:</strong> Edit <code>input</code> in the API call cell: try a vague vs clear question; add \"Context: ‚Ä¶\" then \"Question: ‚Ä¶\"; or add 1‚Äì2 few-shot examples.<br>\n",
        "<strong>Parameters:</strong> Set <code>TEMPERATURE</code> to 0 for deterministic output; to 0.9 for more variety. Set <code>MAX_OUTPUT_TOKENS</code> to 50 for a short reply or 300 for longer. Add <code>INSTRUCTIONS = \"Answer in one short sentence only.\"</code> (or a role like \"You are a Python tutor.\") and re-run.<br>\n",
        "<strong>Context:</strong> In the \"Including more context\" cell, change <code>instructions</code> or <code>context</code> and compare the reply.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Phase 1 complete. Next: Phase 2 (Groq + one MCP).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Exercises\n",
        "\n",
        "*Use only what you learned in this phase (prompts, parameters, instructions, context).*\n",
        "\n",
        "1. **Factual, one-sentence answers**  \n",
        "   You want the model to give **one short, factual answer** with no creativity or extra commentary. What would you set for `temperature` (and optionally `top_p`), and what would you put in `instructions`? Give concrete values or example text.\n",
        "\n",
        "2. **Structured prompt**  \n",
        "   Write a short prompt that includes a **\"Context:\"** block and a **\"Question:\"** block (you can use a toy context and question). In one or two sentences, explain why separating context and question helps the model.\n",
        "\n",
        "3. **Long, rambling replies**  \n",
        "   Suppose the model often returns long, rambling replies and you want shorter ones. Name **two** parameters or prompt changes you could make (from this chapter) and how each would help. No need to write code; just describe the knobs and the effect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional reading\n",
        "\n",
        "**YouTube (verified)**  \n",
        "- [Getting Started with Groq API](https://www.youtube.com/watch?v=S53BanCP14c) ‚Äî Near real-time LLM chat with Groq.  \n",
        "- [Groq API in Python](https://www.youtube.com/watch?v=jScpBCBoGdU) ‚Äî Running generative AI with Groq (popular tutorial).\n",
        "\n",
        "**Blogs (popular)**  \n",
        "- [Groq API Reference](https://console.groq.com/docs) ‚Äî Official docs: models, parameters, token usage.  \n",
        "- [Sampling: temperature, top-k, top-p](https://huyenchip.com/2024/01/16/sampling.html) ‚Äî Chip Huyen: generation configs explained."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://toppng.com/uploads/preview/linkedin-logo-png-photo-116602552293wtc4qogql.png\" width=\"20\" height=\"20\" /> [Bharath Hemachandran](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "# ü§ñ Phase 1: One Groq API Call (No MCP)\n",
        "\n",
        "Learn how **parameters** change the model's output and how **token usage** is reported. Same encode ‚Üí vectors ‚Üí decode as Phase 0 happens on the server; here we focus on **temperature**, **top_p**, **max_output_tokens**, and **truncation**.\n",
        "\n",
        "<div style=\"background: #e3f2fd; padding: 14px; border-radius: 8px; border-left: 4px solid #1976d2;\">\n",
        "<strong>üéØ What you'll do:</strong> Call the Groq Responses API once, tweak parameters, and see how output and <code>input_tokens</code> / <code>output_tokens</code> change.\n",
        "</div>\n",
        "\n",
        "### üìã Notebook objective (table of contents)\n",
        "\n",
        "This notebook covers:\n",
        "- **Setup** ‚Äî Install OpenAI client (Groq-compatible)\n",
        "- **API key** ‚Äî Set GROQ_API_KEY for Colab/local\n",
        "- **Parameters** ‚Äî temperature, top_p, max_output_tokens, truncation, instructions\n",
        "- **API call** ‚Äî Single Groq Responses API request\n",
        "- **Token usage** ‚Äî input_tokens, output_tokens, total_tokens (with a simple chart)\n",
        "- **Output text** ‚Äî Model reply and raw output\n",
        "- **Try it yourself** ‚Äî Suggestions to tweak parameters\n",
        "- **Additional reading** ‚Äî Videos and blogs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup (run once)\n",
        "\n",
        "Install **openai** (Groq is OpenAI-compatible). On Colab, run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîë Set your Groq API key\n",
        "\n",
        "Get a free key at [console.groq.com](https://console.groq.com/keys). In Colab you can use **Secrets** or run the cell below and paste when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass(\"Paste your GROQ_API_KEY: \")\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_groq_client():\n",
        "    return OpenAI(\n",
        "        api_key=os.environ[\"GROQ_API_KEY\"],\n",
        "        base_url=\"https://api.groq.com/openai/v1\",\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Groq client ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è Parameters you can change\n",
        "\n",
        "<div style=\"background: #fff8e1; padding: 12px; border-radius: 8px;\">\n",
        "<strong>Sampling:</strong> <code>temperature</code> (0 = deterministic, 2 = very random), <code>top_p</code> (nucleus sampling).<br>\n",
        "<strong>Length:</strong> <code>max_output_tokens</code> caps the reply length.<br>\n",
        "<strong>Context:</strong> <code>truncation</code> = \"auto\" trims long inputs; \"disabled\" = no trim.<br>\n",
        "<strong>Behavior:</strong> <code>instructions</code> (system) fixes tone or task.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client = get_groq_client()\n",
        "\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 1.0\n",
        "MAX_OUTPUT_TOKENS = 150\n",
        "TRUNCATION = \"disabled\"\n",
        "INSTRUCTIONS = None\n",
        "\n",
        "kwargs = {\n",
        "    \"model\": \"llama-3.3-70b-versatile\",\n",
        "    \"input\": \"In one sentence, what is the Model Context Protocol?\",\n",
        "    \"temperature\": TEMPERATURE,\n",
        "    \"top_p\": TOP_P,\n",
        "    \"max_output_tokens\": MAX_OUTPUT_TOKENS,\n",
        "    \"truncation\": TRUNCATION,\n",
        "}\n",
        "if INSTRUCTIONS is not None:\n",
        "    kwargs[\"instructions\"] = INSTRUCTIONS\n",
        "\n",
        "response = client.responses.create(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Token usage (tokenizing)\n",
        "\n",
        "Input and output are counted in **tokens**, not characters‚Äîsame idea as Phase 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "usage = getattr(response, \"usage\", None)\n",
        "if usage:\n",
        "    inp = getattr(usage, \"input_tokens\", 0)\n",
        "    out = getattr(usage, \"output_tokens\", 0)\n",
        "    tot = getattr(usage, \"total_tokens\", inp + out)\n",
        "    print(\"üìà Token usage:\")\n",
        "    print(f\"   input_tokens:  {inp}\")\n",
        "    print(f\"   output_tokens: {out}\")\n",
        "    print(f\"   total_tokens:  {tot}\")\n",
        "else:\n",
        "    print(\"Usage not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if usage:\n",
        "    inp = getattr(usage, \"input_tokens\", 0)\n",
        "    out = getattr(usage, \"output_tokens\", 0)\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    ax.bar([\"Input\", \"Output\"], [inp, out], color=[\"#1976d2\", \"#388e3c\"], alpha=0.8)\n",
        "    ax.set_ylabel(\"Tokens\")\n",
        "    ax.set_title(\"üìä Input vs output tokens\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì§ Output text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(response.output_text)\n",
        "print(\"\\n--- raw output (first 400 chars) ---\")\n",
        "print(str(response.output)[:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úèÔ∏è Try it yourself\n",
        "\n",
        "<div style=\"background: #e8f5e9; padding: 12px; border-radius: 8px; border-left: 4px solid #4caf50;\">\n",
        "Change <code>TEMPERATURE</code> to 0 and run again for a deterministic reply. Set <code>MAX_OUTPUT_TOKENS</code> to 50 for a shorter answer. Add <code>INSTRUCTIONS = \"Answer in one short sentence only.\"</code> to constrain the model.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚úÖ Phase 1 complete. Next: Phase 2 (Groq + one MCP).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìö Additional reading\n",
        "\n",
        "**YouTube (verified)**  \n",
        "- [Getting Started with Groq API](https://www.youtube.com/watch?v=S53BanCP14c) ‚Äî Near real-time LLM chat with Groq.  \n",
        "- [Groq API in Python](https://www.youtube.com/watch?v=jScpBCBoGdU) ‚Äî Running generative AI with Groq (popular tutorial).\n",
        "\n",
        "**Blogs (popular)**  \n",
        "- [Groq API Reference](https://console.groq.com/docs) ‚Äî Official docs: models, parameters, token usage.  \n",
        "- [Sampling: temperature, top-k, top-p](https://huyenchip.com/2024/01/16/sampling.html) ‚Äî Chip Huyen: generation configs explained."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
